In previous sections we have presented techniques such as batch normalization, dropout, and data augmentation, and claimed that these techniques have been empirically shown to combat overfitting and/or decrease training times.
Unfortunately, machine learning techniques are often highly context dependent with respect to their efficiency, and this section is therefore intended as a verification of these techniques in the context of remote sensing data and building footprint segmentation.

\topic{Batch normalization}

We have trained two U-Net models on LiDAR data, one \emph{with} batch normalization and one \emph{without}, the training procedure of both these models being presented in~\figref{fig:batch-normalization-training}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.495\linewidth]{metrics/lidar_without_batch_normalization+without_rgb-train+validation-iou}
  \includegraphics[width=0.495\linewidth]{metrics/lidar_without_batch_normalization+without_rgb-train+validation-loss}
  \caption{%
    Training procedure of two U-Net models on LiDAR data, one employed with batch normalization shown in \textcolor{blue}{blue}, while the other has no batch normalization and shown in \textcolor{orange}{orange}.
    Left panel shows the IoU metric evaluations, while the right panel shows the binary cross-entropy loss.
  }%
  \label{fig:batch-normalization-training}
\end{figure}
\vspace{-\baselineskip}
The comparative performance improvement of the batch normalized model over the model without batch normalization becomes immediately clear from~\figref{fig:batch-normalization-training}.
What is of particular interest is that the batch normalization does not only increase the speed of optimizing the loss function, but it also improves the final model performance in form of validation IoU.
Of all the A/B tests conducted in this section, this test has had the most significant effect.

\topic{Dropout}

As with the batch normalization experiment, we now train one model \emph{with} max-pooling dropout and one \emph{without}, and the training results are presented in~\figref{fig:dropout-training}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.495\linewidth]{metrics/lidar_without_dropout+without_rgb-validation-iou}
  \includegraphics[width=0.495\textwidth]{iou_distribution/lidar_without_dropout}
  \caption{%
    \textbf{Left --} Training procedure of two U-Net models on LiDAR data, one employed with max-pooling dropout shown in \textcolor{blue}{blue}, while the other uses no max-pooling dropout and is shown in \textcolor{orange}{orange}.
    \textbf{Right --} Test IoU distribution of LiDAR model without dropout, left \SI{6}{\percent} of data cropped.
    See caption of~\figref{fig:iou-distribution-explanation} for detailed description.
  }%
  \label{fig:dropout-training}
\end{figure}

While the improvement of batch normalization was immediately obvious, the effect of dropout is more difficult to interpret from the left panel of~\figref{fig:dropout-training} alone as we only observe a marginal IoU validation metric improvement from \num{0.9335} to \num{0.9363}.
On the other hand, the right panel of~\figref{fig:dropout-training} shows that the interquartile range and median of the test IoU metrics are identical across the two models, but that the right-end tail of the distribution has grown thicker.
Methods preventing overfitting will hypothetically bridge the gap between the training- and test-evaluation of the model.
As dropout is primarily intended as a measure to prevent overfitting, we will investigate the performance of both models on the training set and compare this with their performance on the test set.
\figref{fig:dropout-train-test} presents a comparison of these two models, a plot similar to the one shown in~\figref{fig:correlation-explanation}.
\vspace{-\baselineskip}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\linewidth]{metric_correlation/without_rgb+lidar_without_dropout+iou}
  \vspace{-\baselineskip}
  \caption{%
    Scatter plot showing the correlation between the evaluation metric performance of two models, one with max-pooling dropout and one without.
    The model without dropout is shown along the vertical axis, while the model with dropout is shown along the horizontal axis.
    See caption of~\figref{fig:correlation-explanation} for detailed figure explanation.
  }%
  \label{fig:dropout-train-test}
\end{figure}

The U-Net model \emph{without} dropout largely outperforms the model with dropout on the \emph{training} set, having a better IoU evaluation on about 98\% of all training tiles and a mean training IoU of \num{0.958}, which is substantially better than the dropout model with a mean training IoU of \num{0.946}.
What is of particular interest, though, is that the dropout model generalizes \emph{much} better to the test tile set, so much so that it narrowly outperforms the model without dropout on the test set.
The 98\% / 2\% split is reduced to a more even 53\% / 47\% split, making the two models approximately tied.
The mean test IoU of the dropout model, \num{0.922}, is also better than the mean test IoU of the non-dropout model, \num{0.918}.
Altogether this can be considered quite strong evidence in favor of max-pooling dropout having reduced the overfitting of our model.
The application of dropout during model training likely increases the generalizability of our model as it improves its ability to predict building outlines from previously unseen remote sensing data.

\topic{Data Augmentation}

Finally we investigate the effect of data augmentation when training the LiDAR model, comparisons of the two models being presented in~\figref{fig:data-augmentation-experiment}.
The data augmentation consists of random application of horizontal and/or vertical flipping in addition to a rotation by a random multiple of 90 degrees, resulting in altogether 16 random configurations of each training tile.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.495\linewidth]{metrics/lidar_without_augment+without_rgb-validation-iou}
  \includegraphics[width=0.495\textwidth]{iou_distribution/lidar_without_augment}
  \includegraphics[width=0.55\linewidth,trim={0 0.4cm 0 0.9cm},clip]{metric_correlation/without_rgb+lidar_without_augment+iou}
  \hspace{-0.9em}
  \caption{%
    \textbf{Top left --} Training procedure of two U-Net models on LiDAR data, one employed with data augmentation shown in \textcolor{blue}{blue}, while the other uses no data augmentation and is shown in \textcolor{orange}{orange}.
    \textbf{Top right --} Test IoU distribution of model trained without data augmentation, left \SI{5}{\percent} of data cropped.
    See caption of~\figref{fig:iou-distribution-explanation} for detailed description.
    \textbf{Bottom --} Scatter plot showing the correlation between the evaluation metric performance of two models, one with data augmentation (horizontal) and one without (vertical).
    See caption of~\figref{fig:correlation-explanation} for detailed figure explanation.
  }%
  \label{fig:data-augmentation-experiment}
\end{figure}

No significant difference between the two training schemes can be observed in~\figref{fig:data-augmentation-experiment}.
The observed differences between the two resulting models can as likely be attributed to random noise than any causal effect of the data augmentation procedure.
The augmentation techniques applied during training, namely flipping and 90 degree rotations, have intentionally been selected in order to be negligible in computational cost.
They do not require any de facto calculations as they can be implemented by simply traversing the memory layout in a different manner, and are therefore constant $\mathcal{O}(1)$ time cost operations.
Considering that data augmentation is cheaply performed and we have no evidence contrary to it being a positive influence of the generalizability of the model, we conclude that data augmentation should be performed during training.
Finally, it is worth noting that the data augmentations applied in this case are rather simple and minor forms of data augmentation.
More major augmentation forms might have a bigger effect.

% If the subset of labeled observations used for training possesses certain systematic properties that are not necessarily present in the universal set of future cases to be predicted, data augmentation can be used in order to \enquote{prepare} the model for such cases.
% Consider a hypothetical region A where buildings are overwhelmingly built such that the longest building edge faces southwards, while buildings in hypothetical region B does not follow this pattern.
% A model trained on data sourced from region A would likely perform better when making predictions in region B if rotational augmentation is performed during training.
% Arguably rotation and flipping of the LiDAR input tiles corrects a relatively small subset of all systematic properties.
% Aerial RGB photography can be augmented by contrast and saturation adjustments, for instance, augmentation methods which we have not implemented.
% We hypothesize that the effect of augmentation is likely greater for aerial RGB imagery rather than LiDAR data, especially if the validation and test set are captured under different weather and lighting conditions.
