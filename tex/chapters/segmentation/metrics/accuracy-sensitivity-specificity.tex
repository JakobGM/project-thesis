For ground truths in classification problems, we have the following definitions:

\begin{itemize}[noitemsep]
  \item \textbf{Condition Positive (P)} - Number of class $A$ in the data.
  \item \textbf{Condition Negative (N)} - Number of class $A^C$ in the data.
\end{itemize}

Where $A^C$ denotes the class complement of $A$, i.e. all other classes besides class $A$.
In our binary classification problem $A$ denotes $1$ and $A^C$ denotes $0$.

Four related concepts need also be defined:

\begin{itemize}[noitemsep]
  \item \textbf{True Positive (TP)} - Class $A$ correctly predicted as $A$ (correctly identified).
  \item \textbf{True Negative (TN)} - Class $A^C$ correctly predicted as $A^C$ (correctly rejected).
  \item \textbf{False Positive (FP)} - $A^C$ wrongly predicted as $A$ (incorrectly identified).
  \item \textbf{False Negative (FN)} - $A$ wrongly predicted as $A^C$ (incorrectly rejected).
\end{itemize}

False positives (FP) are often knows as \textit{type I errors} in statistics, and false negatives (FN) as \textit{Type II errors}.
The greater the values of TP and TN, the better, and the smaller the values of FP and FN, the better. We will collectively refer to these four values as \textit{confusions}, for a lack of a better name.

\begin{figure}[htb]
  \includegraphics[width=\linewidth]{confusions}
  \caption{
    Binary segmentation problem of size $256 \times 256$.
    The ground truth, a rectangle of size $120 \times 80$ is shown on the left.
    The \enquote{predicted} mask, shown in the middle, is of the same size, but offset by $(-30, -30)$.
    The right figure shows the visual equivalent of a confusion matrix.
    True positives are shown in \textcolor{tp}{dark blue}, true negatives in \textcolor{gray}{light gray}, false positives in \textcolor{fp}{green}, and false negatives in \textcolor{fn}{red}.
  }
  \label{fig:confusions}
\end{figure}

A more naive metric for semantic segmentation problems is the \textit{pixel accuracy}.
This metric simply reports the percentage of pixels that were correctly classified.
More formally, it can be defined as:

\begin{equation*}
    \textbf{accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{P + N}
\end{equation*}

The problem with pixel-wise accuracy metrics is that it does not account for class imbalances.
Consider a problem where 95\% of all pixels are considered to be of class $0$, and the remaining 5\% of class $1$.
If we construct a model which predicts `0.0` regardless of the feature inputs provided to the model, the model will achieve a 95\% accuracy score.
Generally, a model which predicts always predicts `0` on a test set with class balance $\alpha / (1 - \alpha)$ will achieve a pixel-wise accuracy score of $\alpha$.

This makes pixel-wise accuracy scores basically useless when you do not know the class balance of the underlying dataset and the actual predictions across the classes.
This is why it is often replaced by other metrics which takes imbalances and model confidence into account.

One of the issues with the confusion metrics, is that it is not \textit{sample size invariant}.
With other words, it is not straight forward to compare two confusion matrices when the metrics have been calculated of two datasets of different size. The concepts of \textit{sensitivity} and \textit{specificity} helps with this comparison problem:

\begin{align*}
    \textbf{sensitivity}
    &=
    \frac{\text{number of true positives}}{\text{number of true positives + number of false negatives}}
    =
    \frac{TP}{TP + FN}
    =
    \frac{TP}{P}
    \\
    \textbf{specificity}
    &=
    \frac{\text{number of true negatives}}{\text{number of true negatives + number of false positives}}
    =
    \frac{TN}{TN + FP}
    =
    \frac{TN}{N}
\end{align*}

The \textit{sensitivity} is therefore a measure of how good the given model prediction was able to identify positives as a relative, fractional value.
Likewise, the \textit{specificity} is a measure of how good the given model prediction was able to identify negatives as a relative, fractional value.
