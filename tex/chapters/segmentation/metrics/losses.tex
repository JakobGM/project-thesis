So far we have only discussed metrics which are discrete, non-differentiable functions, thus making them unsuitable for direct optimization.
As discussed earlier, we need to introduce a differentiable surrogate loss function which we can optimize.
The key point is to define a loss function in the continuous probability domain instead of the discrete binary classification domain.
In order to formulate proper loss functions, we will start by establishing some notation.
Denote the ground truth binary classification mask as $Y \in \mathbb{B}^{H \times W}$ and the corresponding features as $X \in \mathbb{R}^{H \times W \times C}$.
Assume a model $\hat{f}$ parametrized according to $\vec{\theta}$ which provides a probability estimate for $Y$, the probability estimate denoted as $P = \hat{f}(X; \vec{\theta}) \in [0, 1]^{H \times W}$.
For notational simplicity we will use a linear index in order to denote single matrix elements, for instance $P_i \in [0, 1]$ for $i = 1, \ldots, HW$.

The most common loss function for binary classification tasks is the \textit{binary cross entropy} (BCE) loss function defined as
%
\begin{equation*}
  \mathcal{L}_{\mathrm{BCE}}(P; Y)
  =
  - \sum\limits_{i = 1}^{HW}
  Y_i \log{(P_i)}
  +
  (1 - Y_i) \log{(1 - P_i)}.
\end{equation*}
%
However, there are several issues with using the BCE as the loss function for segmentation tasks.
Firstly, it does not take class imbalances into account.
The \textit{weighted binary cross entropy} (wBCE) is one attempt at accounting for class imbalances, but weighting is highly task-dependent and has been shown to have negligible performance improvement over BCE~\cite[p.~98]{soft-losses}.
Another issue is that BCE and wBCE are poor surrogates for the segmentation metrics introduced earlier, metrics which \textit{actually} want to maximize.
The solution is to introduce differentiable approximations of these discrete segmentation metrics.
Such an approximation for the dice coefficient is the so-called \textit{soft dice loss} first introduced by~\citeauthor{original-soft-dice-loss}~\cite{original-soft-dice-loss}.
This differentiable loss function is defined by
%
\begin{equation*}
  \mathcal{L}_{\mathrm{SDL}}(P; Y)
  =
  \frac{%
    2 \sum\limits_{i = 1}^{HW}
    P_i Y_i
  }{%
    \sum\limits_{i = 1}^{HW} P_i^2
    +
    \sum\limits_{i = 1}^{HW}  Y_i^2
    +
    \epsilon
  }
  \approx
  1 - F_1
\end{equation*}
%
A similar differentiable approximation of the IoU metric, called \textit{soft Jaccard loss}, has also been derived~\cite{soft-jacard-loss}.
Optimizing these two metric-sensitive losses have been shown theoretically and empirically to indirectly maximize their respective surrogate metrics~\cite{soft-losses}.
You would think that if the IoU metric has been chosen as the metric of interest for a given problem, the soft Jaccard loss should be used instead of soft dice loss.
However, \citeauthor{soft-losses} have shown~\cite{soft-losses} that these two metric-sensitive losses are equally good approximations for the dice coefficient as well as the IoU metric, and the choice is therefore a preferential one.
