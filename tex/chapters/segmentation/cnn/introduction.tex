There exists countless variations of the CNN model architecture, but there are still some elementary building blocks which they often have in common.
We will start by sketching generic big picture of CNNs before going into detail about each modular building block.
\figref{fig:unet} illustrates the architecture of \textit{U-Net}, and we will use this figure to illustrate the common concepts of segmentation CNNs without considering the unique properties of U-Net specifically.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{unet}
  \caption{%
    Illustration of the U-Net architecture for single-class segmentation, a typical example of an \textit{encoder/decoder} structure.
    Convolution layers are shown in orange, and max pooling layers in red.
    Arrows indicates how data is forwarded through the network, top arrows being \textit{skip connections}.
    The right hand side shows the upscaling performed by \textit{transposed convolution} until the original resolution is restored and segmentation predictions can be formed with the \textit{sigmoid} activation function (shown in purple).
    Figure has been generated by modifying a \texttt{tikz} example provided in the MIT licenced \texttt{PlotNeuralNet} library available at this URL:\@ \\ % chktex 13
    \protect\url{https://github.com/HarisIqbal88/PlotNeuralNet}.
  }%
  \label{fig:unet}
\end{figure}

A CNN consists of several layered blocks operating over identical input dimensions within each block.
These blocks are shown as contiguous boxes in~\figref{fig:unet}.
The first layer in each block is a \textit{convolutional layer}, which is a type of trainable feature extraction where several filtered \textit{feature maps} are constructed.
Each feature map is passed through a nonlinear \textit{activation function} and the \textit{activations} are subsequently downsampled in order to reduce the resolution.
The downsampling is performed by a \textit{pooling layer} and the output is forwarded to the next block.
The number of feature maps that are extracted from the previous pooled activations increases as the resolution is decreased, and the right half of the architecture is eventually responsible for upsampling the resolution back to the original resolution by the means of \textit{deconvolution}.
The upsampling half of this network is not common to all CNNs, as CNNs tasked with bounding box regression and classification are not required to restore the original resolution before prediction.
The upcoming sections will describe these concepts in more detail.
