# Guidance session 5

## Done since last time

* Attempting to use U-Net model from last session on my data set.

## Questions for session

* Difference between `Conv2DTranspose` and `UpSampling2D`?
* How does `dtype` affect performance of TensorFlow training, if at all?
* How to best do hyperparameter search? Dropout rate for instance?
* Binary cross entropy is probably a sub-optimal loss function.
  Perhaps implement one of the loss functions mentioned in
  [this article](https://www.jeremyjordan.me/semantic-segmentation/#loss)?
  Soft dice loss perhaps?

## Session notes

* Should take a look at different loss functions and how they affect the result.
  * A comparative summary, contrasting the different loss functions should
    be done.
* Multi-task learning can be used in order to take the problem a step further.
  Use this in order to classify individual roof surfaces.
* Add auxiliary input to network related to scaling of that specific tile
  * The scaling is global for the entire image
  * Adding an additional channel with duplicated values will be quite redundant.
  * Perhaps add scaling input into the last (second to) convolutional layer?
  * In order to make the model more robust, input scaling as a quantile of the
    scaling distribution instead of the real absolute scaling value?
* Individual surface problem
  * Project surfaces into two dimensional plane and label accordingly.
  * Could either do edge detection or instance segmentation.
  * `Polygon`, `LineString`, and `ClosedLineString` may be discretized
    in the two dimensional space, and these pixels are the new positive
    labels.
* Scaling problem
  * Scaling each individual tile to the domain (0, 1) is not necessarily too
    bad. Information is lost in most normalization techniques anyway.
  * Perhaps a constant scaler for the common case, and a dynamic (0, 1)
    scaler for the extreme cases. For example divide by `C` as long
    as `delta := (max(Z) - min(Z)) <= C`, otherwise divide by `delta`.
* Some sort of hyperparameter search should be done before finishing up
  the work. The [keras-tuner project](https://github.com/keras-team/keras-tuner)
  might be suitable for this purpose.
* Tips for writing
  * The topic is quite applied, so I could/should write a lot about the
    practicalities, such as data management, data preprocessing, and
    so on. Quite a lot of time is invested in this part, and it should
    be reflected in the final text.
  * When I have ideas and hypotheses, I should write them down right
    away. It is easy to forget about the original motivations behind
    the final implementations two months later when I am finishing
    the textual content of the thesis.
* Next on the agenda
  * Implement a method which shows the `n` tiles which has the worst
    evaluation metric. This way I can see if there are any easy wins
    to get. Hopefully this will reveal how the current (0, 1) scaling
    scheme may screw up in certain extreme scenarios.
  * Perform training/validation/testing on all the data that is available.
  * Look at different loss functions.
  * Add RGB channels to input images.
    * Is less data required for a good model compared to when the RGB
      is not included? Compare these two models!
  * New problem: find ridge lines
* Current idea of success criteria
  * Implement model which uses 4 channels, ZRGB.
  * Ridge detection.
  * When these two are finished, it might be enough for a good project
    thesis.
