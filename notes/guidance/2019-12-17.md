# Guidance session 12

## Questions

* Where should I describe the U-Net model architecture and data augmentation?
* How should models be compared in the experiments section? Validation set or test set?
* Is it possible to distinguish between performance difference due to randomness versus actual improvement?
* Should "future work" be a separate section?
* What about training the same model on the exact same data and see how random it is?
* Can I do any sort of ablation experiment here?
* "Dynamic scaling" / "Constant scaling" the best labels?
* A good statistic for discrepancy between x and y model? MSE?
* Should I write a section about the software being used? TensorFlow 2, docker, GDAL, and so on?
* Should I write about the training time? If so, I should probably write about the technical specs of the computer as well?

## TODO

* Training additional LiDAR model to see how much randomness is involved
* Train LiDAR model with no area filter
* What does the model predict on a zero matrix? What about 1s in the middle? Normal noise instead of 0s?
* Ablation experiment for RGB+LiDAR model. What happens when RGB tiles are interchanged?
* All losses in one plot
* Plot correlation between mask percentage and IoU in test set, scatter plot for instance
* Investigate outliers soft loss models in test split. What is going on?
* Color each dot in metric correlation according to percentage mask?
* Add all types of prediction plots
* Plot range against IoU for both normalization methods
* Visualize feature activations: https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/
* Add looking glass effect for focusing on problem areas
