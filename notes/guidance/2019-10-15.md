# Guidance session 7

## Session Notes

* A maximum batch size of 16 before going OOM is quite low when we have 7 million trainable parameters. Batch sizes of 100 is not uncommon. This should be looked into at some point, but is not of high priority at the moment.
* Tips for improving the model:
  * [RAdam optimizer](https://github.com/CyberZHG/keras-radam)
  * [Lookahead mechanism for optimizer](https://github.com/CyberZHG/keras-lookahead)
  * [Mish activation function](https://github.com/digantamisra98/Mish)
  * Good optimizers / activation functions can help with preventing overfitting when we have larger batch sizes, allowing us to train faster without losing accuracy.
* The plan going forwards:
  * [ ] Investigate the issue of MeanIoU metric often being capped from above at 0.5. Why is this happening? Is something wrong?
  * [ ] Set up TensorBoard in order to save pertinent information of the training. This should be easily retrievable by name in order to compare models. Final weights, full model architecture, training/validation loss/metric history, and so on.
  * [ ] Train models with RGB, LiDAR, and both. Contrast and compare.
  * [ ] Look into new techniques and problems when we are done with this.
